# -*- coding: utf-8 -*-
"""similarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rrM2aekthto338CJ0If0N5_6MiHTFQnW
"""

from transformers import BertTokenizer, BertModel
import torch
from sklearn.metrics.pairwise import cosine_similarity

# Load tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Sentences to compare
sentence1 = "I love cats"
sentence2 = "I like animals"

# Tokenize and convert to input tensors
inputs1 = tokenizer(sentence1, return_tensors='pt', padding=True, truncation=True)
inputs2 = tokenizer(sentence2, return_tensors='pt', padding=True, truncation=True)

# Get embeddings
with torch.no_grad():
    outputs1 = model(**inputs1).last_hidden_state.mean(dim=1)
    outputs2 = model(**inputs2).last_hidden_state.mean(dim=1)

# Calculate cosine similarity
similarity = cosine_similarity(outputs1, outputs2)[0][0]

print(f"Similarity between the sentences: {similarity:.2f}")