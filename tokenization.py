# -*- coding: utf-8 -*-
"""tokenization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RMIS5kg-kDuks9cRvf5lhVcEegzB98Lc
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

texts=["I Love Deep Learning","RNNs Are Powerful For Sequence Data","Tokenization Is Very Important For NLP"]

tokenizer = Tokenizer()

tokenizer.fit_on_texts(texts)
sequence=tokenizer.texts_to_sequences(texts)
padded_sequence=pad_sequences(sequence,padding='post')

print("Word index:\n",tokenizer.word_index)
print("\nSequence:\n",sequence)
print("\nPadded Sequence:\n",padded_sequence)

padded_sequence=pad_sequences(sequence,padding='pre')

print("Word index:\n",tokenizer.word_index)
print("\nSequence:\n",sequence)
print("\nPadded Sequence:\n",padded_sequence)

word_tokens = [text.split() for text in texts]

print(word_tokens)

char_tokens = [list(text) for text in texts]

print(char_tokens)