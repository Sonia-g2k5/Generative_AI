# -*- coding: utf-8 -*-
"""question_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bo6JN4ezacXU2c7DhG97261w9PmiZitn
"""

from transformers import AutoTokenizer, AutoModelWithLMHead
import torch

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("mrm8488/t5-base-finetuned-question-generation-ap")
model = AutoModelWithLMHead.from_pretrained("mrm8488/t5-base-finetuned-question-generation-ap")

# Function to generate a question
def generate_question(answer: str, context: str, max_length: int = 64) -> str:
    # Prepare input in the required format
    input_text = f"answer: {answer}  context: {context} </s>"
    inputs = tokenizer([input_text], return_tensors="pt")

    # Generate output
    output_ids = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_length=max_length,
        num_beams=4,
        early_stopping=True
    )

    # Decode the output
    question = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return question

# Example usage
context = "i am sonia and i love ice cream"
answer = "sonia"

question = generate_question(answer, context)
print("Generated Question:", question)
print("Done by Sonia \n")